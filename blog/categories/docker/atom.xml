<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Marcus Young]]></title>
  <link href="https://marcyoung.us/blog/categories/docker/atom.xml" rel="self"/>
  <link href="https://marcyoung.us/"/>
  <updated>2020-06-08T14:53:03+00:00</updated>
  <id>https://marcyoung.us/</id>
  <author>
    <name><![CDATA[Marcus Young]]></name>
    
  </author>
  <generator uri="https://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Playing around with Gatekeeper V3 in K8S]]></title>
    <link href="https://marcyoung.us/post/gatekeeper-v3"/>
    <updated>2019-12-02T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/opa-k8s</id>
    <content type="html"><![CDATA[<p>Enforce allow/deny Policies in kubernetes at a base controller level  <!-- more --></p>

<p>I was at kubecon the in mid-November with an old roommate (in the same industry as me) and some coworkers but got to see some cool kubernetes stuff.</p>

<p>Since my job crosses the Security boundary, and I&rsquo;m fairly new to k8s, most of my talks were somewhere on the basic level or <a href="https://github.com/open-policy-agent/opa">opa level</a>. With OPA what you get is a lightweight language to write policies your kubernetes cluster should adhere to.</p>

<p>Typically (I think) this is traditionally run by building your policies into a sidecar and forcing it to be an <a href="https://github.com/open-policy-agent/kube-mgmt">admission controller</a>.</p>

<p>Gatekeeper simplifies this by removing the side-car for native CRD&rsquo;s and adds a few things like audit logs.</p>

<p>I&rsquo;m going to build up an example of this using EKS.</p>

<p>I assume we have a working EKS cluster:</p>

<pre><code>$ aws-vault exec home -- kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   172.20.0.1   &lt;none&gt;        443/TCP   6h15m
</code></pre>

<p>Before we deploy anything, let&rsquo;s set up GateKeeper v3:</p>

<pre><code>$ aws-vault exec home -- kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
</code></pre>

<p>Gatekeeper should now be running (albeit with no policies):</p>

<pre><code>$ aws-vault exec home -- kubectl get all -n gatekeeper-system
NAME                                  READY   STATUS    RESTARTS   AGE
pod/gatekeeper-controller-manager-0   1/1     Running   1          144m

NAME                                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/gatekeeper-controller-manager-service   ClusterIP   172.20.41.90   &lt;none&gt;        443/TCP   144m

NAME                                             READY   AGE
statefulset.apps/gatekeeper-controller-manager   1/1     144m
</code></pre>

<p>Next, before we move on, I&rsquo;m going to set up <a href="https://github.com/garethr/policykit">policykit</a> so that we can use real REGO (the language for OPA) instead of rego embedded inside yaml (allows policy testing etc later on):</p>

<pre><code>$ pip install policykit
$ cat &lt;&lt;EOF &gt;k8sallowedrepos.rego
package k8sallowedrepos

violation[{"msg": msg}] {
  container := input.review.object.spec.containers[_]
  satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
  not any(satisfied)
  msg := sprintf("container &lt;%v&gt; has an invalid image repo &lt;%v&gt;, allowed repos are %v", [container.name, container.image, input.parameters.repos])
}

violation[{"msg": msg}] {
  container := input.review.object.spec.initContainers[_]
  satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
  not any(satisfied)
  msg := sprintf("container &lt;%v&gt; has an invalid image repo &lt;%v&gt;, allowed repos are %v", [container.name, container.image, input.parameters.repos])
}
EOF
</code></pre>

<p>What we have now is a rego file that will validate base images (parameterized). Let&rsquo;s write some tests.</p>

<pre><code>$ cat &lt;&lt;EOF &gt;k8sallowedrepos_test.rego
package k8sallowedrepos

test_image_safety_positive {
    count(violation) == 1 with input.parameters.repos as ["hooli.com/"]
        with input.review.object.spec.containers as [
            {"name": "ok", "image": "hooli.com/web"},
            {"name": "bad", "image": "badrepo.com/web"},
        ]
}

test_image_safety_negative {
    count(violation) == 0 with input.parameters.repos as ["hooli.com/"]
        with input.review.object.spec.containers as [
            {"name": "ok", "image": "hooli.com/web"},
        ]
}

test_image_safety_init_container_positive {
    count(violation) == 1 with input.parameters.repos as ["hooli.com/"]
        with input.review.object.spec.initContainers as [
            {"name": "ok", "image": "hooli.com/web"},
            {"name": "bad", "image": "badrepo.com/web"},
        ]
}

test_image_safety_init_container_negative {
    count(violation) == 0 with input.parameters.repos as ["hooli.com/"]
        with input.review.object.spec.initContainers as [
            {"name": "ok", "image": "hooli.com/web"},
        ]
}
EOF

$ curl -Ls https://github.com/open-policy-agent/opa/releases/download/v0.15.1/opa_linux_amd64 -o opa &amp;&amp; chmod +x opa
$ opa test . -v
data.k8sallowedrepos.test_image_safety_positive: PASS (622.8µs)
data.k8sallowedrepos.test_image_safety_negative: PASS (376µs)
data.k8sallowedrepos.test_image_safety_init_container_positive: PASS (550.6µs)
data.k8sallowedrepos.test_image_safety_init_container_negative: PASS (389.3µs)
--------------------------------------------------------------------------------
PASS: 4/4
</code></pre>

<p>Now let&rsquo;s generate our ConstraintTemplate (the basic template that accepts parameters):</p>

<pre><code>$ pk build *.rego
[k8sallowedrepos] Generating a ConstraintTemplate from "k8sallowedrepos.rego"
[k8sallowedrepos] Saving to "k8sallowedrepos.yaml"
</code></pre>

<p>Let&rsquo;s see what that ConstraintTemplate looks like:</p>

<pre><code>$ cat k8sallowedrepos.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: k8sallowedrepos
        listKind: k8sallowedreposList
        plural: k8sallowedrepos
        singular: k8sallowedrepo
  targets:
  - rego: |
      package k8sallowedrepos

      violation[{"msg": msg}] {
        container := input.review.object.spec.containers[_]
        satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
        not any(satisfied)
        msg := sprintf("container &lt;%v&gt; has an invalid image repo &lt;%v&gt;, allowed repos are %v", [container.name, container.image, input.parameters.repos])
      }

      violation[{"msg": msg}] {
        container := input.review.object.spec.initContainers[_]
        satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
        not any(satisfied)
        msg := sprintf("container &lt;%v&gt; has an invalid image repo &lt;%v&gt;, allowed repos are %v", [container.name, container.image, input.parameters.repos])
      }
    target: admission.k8s.gatekeeper.sh
</code></pre>

<p>As you can see that rego is now embedded inside a ConstraintTemplate yaml. Let&rsquo;s apply it:</p>

<pre><code>$ aws-vault exec home -- kubectl apply -f k8sallowedrepos.yaml
constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos configured
</code></pre>

<p>We have a template but no OPA rules yet.</p>

<pre><code>$ aws-vault exec home -- kubectl logs pod/gatekeeper-controller-manager-0 -n gatekeeper-system
{"level":"info","ts":1575315679.2535567,"logger":"controller","msg":"Audit opa.Audit() audit results","metaKind":"audit","violations":0}
{"level":"info","ts":1575315679.255231,"logger":"controller","msg":"constraint","metaKind":"audit","resource kind":"k8sallowedrepos"}
{"level":"info","ts":1575315679.259529,"logger":"controller","msg":"constraint","metaKind":"audit","count of constraints":0}
</code></pre>

<p>Let&rsquo;s add a rule to deny anything that&rsquo;s not from <code>alpine</code> (ironically the opposite of secure):</p>

<pre><code>$ cat &lt;&lt;EOF &gt;check_repo.yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: k8sallowedrepos
metadata:
  name: default-repo-is-wrong
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - "default"
  parameters:
    repos:
      - "alpine"
EOF
$ aws-vault exec home -- kubectl apply -f check_repo.yaml
k8sallowedrepos.constraints.gatekeeper.sh/default-repo-is-wrong created
</code></pre>

<p>We can now see Gatekeeper has it:</p>

<pre><code>...snip noise...
{"level":"info","ts":1575315769.4250739,"logger":"controller","msg":"constraint","metaKind":"audit","count of constraints":1}
...snip noise...
</code></pre>

<p>Let&rsquo;s push up a Helm chart that violates it:</p>

<pre><code>$ cat charts/cloud-custodian/values.yaml  | grep repos
  repository: 11111111111.dkr.ecr.us-east-1.amazonaws.com/cloud-custodian
$ helm package charts/cloud-custodian/
Successfully packaged chart and saved it to: charts/cloud-custodian-0.1.2.tgz
$ aws-vault exec home -- helm install --name cloud-custodian charts/cloud-custodian-0.1.2.tgz
NAME:   cloud-custodian
LAST DEPLOYED: Mon Dec  2 13:44:47 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Deployment
NAME             READY  UP-TO-DATE  AVAILABLE  AGE
cloud-custodian  0/1    0           0          0s

==&gt; v1/ServiceAccount
NAME             SECRETS  AGE
cloud-custodian  1        0s
</code></pre>

<p>Did it work?</p>

<pre><code>$ aws-vault exec home -- kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   172.20.0.1   &lt;none&gt;        443/TCP   6h25m

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cloud-custodian   0/1     0            0           18s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cloud-custodian-744bd4768d   1         0         0       18s
</code></pre>

<p>It&rsquo;s not running. let&rsquo;s look:</p>

<pre><code>$ aws-vault exec home -- kubectl describe replicaset.apps/cloud-custodian-744bd4768d | tail -n 4
Events:
  Type     Reason        Age                From                   Message
  ----     ------        ----               ----                   -------
  Warning  FailedCreate  3s (x14 over 44s)  replicaset-controller  Error creating: admission webhook "validation.gatekeeper.sh" denied the request: [denied by default-repo-is-wrong] container &lt;cloud-custodian&gt; has an invalid image repo &lt;11111111111.dkr.ecr.us-east-1.amazonaws.com/cloud-custodian:latest&gt;, allowed repos are ["alpine"]
</code></pre>

<p>And just like that we enforced a policy that prevented this from running!</p>

<p>More to come from OPA as I get more comfortable with it and learn how to test the policies before they go into K8S!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vault as a CA for ECS containers using Terraform (Part 2)]]></title>
    <link href="https://marcyoung.us/post/docker-vault-ca-part2"/>
    <updated>2017-12-24T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/docker-vault-ca-part2</id>
    <content type="html"><![CDATA[<p>Now for some cool snippets on how to automatically request certs from the Intermediary we created in Part 1<!-- more --></p>

<h2>Dockerfile</h2>

<p>An example of how I typically get Vault into my base Container:</p>

<pre><code>FROM debian:jessie
ENV VAULT_VERSION 0.8.1
ENV DUMBINIT_VERSION 1.2.0

RUN apt-get update &amp;&amp; apt-get install -y \
        apt-transport-https \
        ca-certificates \
        wget \
        unzip \
        jq \
        curl \
    nginx \
    --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN curl -L --silent -o vault.zip https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip &amp;&amp; \
  unzip vault.zip &amp;&amp; \
  mv vault /usr/local/bin/vault &amp;&amp; \
  chmod +x /usr/local/bin/vault

RUN curl -L --silent -o /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v${DUMBINIT_VERSION}/dumb-init_${DUMBINIT_VERSION}_amd64 &amp;&amp; \
  chmod +x /usr/local/bin/dumb-init

RUN mkdir -p /opt/ssl
COPY docker-entrypoint.sh /
RUN chmod +x /docker-entrypoint.sh
ENTRYPOINT ["/usr/local/bin/dumb-init", "--"]
CMD ["/docker-entrypoint.sh"]
</code></pre>

<h2>The Entrypoint</h2>

<pre><code>#!/bin/bash

export SSL_PATH=/opt/ssl

EC2_AVAIL_ZONE=$(curl -s --max-time 5 https://169.254.169.254/latest/meta-data/placement/availability-zone)
if [[ $? -eq 0 ]]; then
  # shellcheck disable=SC2001,SC2006
  EC2_REGION="`echo \"$EC2_AVAIL_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"
  export AWS_DEFAULT_REGION=$EC2_REGION
  export AWS_REGION=$EC2_REGION
fi

[[ -z "${VAULT_TOKEN}" ]] &amp;&amp; vault auth -method=aws &gt;/dev/null

VAULT_JSON="$(vault write -format=json cuddletech_ops/issue/jenkins common_name=jenkins ttl=15551999)"

echo "$VAULT_JSON" | jq -r .data.private_key &gt; ${SSL_PATH}/key.pem
echo "$VAULT_JSON" | jq -r .data.certificate &gt; ${SSL_PATH}/cert.pem
echo "$VAULT_JSON" | jq -r .data.issuing_ca  &gt; ${SSL_PATH}/ca.pem
cat ${SSL_PATH}/cert.pem ${SSL_PATH}/ca.pem  &gt; ${SSL_PATH}/bundle.pem

# run your command and point to the certs generated above. Example in nginx:
#        ssl                  on;
#        ssl_certificate      /opt/ssl/bundle.pem;
#        ssl_certificate_key  /opt/ssl/key.pem;
</code></pre>

<h2>Terraforming the certificate issuer</h2>

<p>If you somehow get the previous part to run, it will fail because you have to create the role in Vault before you can just start generating certs off it. In the <a href="https://cuddletech.com/?p=959">cuddletech guide</a> this is under <code>Requesting a Certificate for a Web Server</code> as:</p>

<pre><code>$ vault write cuddletech_ops/roles/web_server \
&gt; key_bits=2048 \
&gt; max_ttl=8760h \
&gt; allow_any_name=true
Success! Data written to: cuddletech_ops/roles/web_server
</code></pre>

<p>But what if I told you, you could do that in terraform?</p>

<pre><code>variable "role" {
}

resource "vault_generic_secret" "pki_role" {
  path = "cuddletech_ops/roles/${var.role}"

  data_json = &lt;&lt;EOT
{
  "key_bits": "2048",
  "max_ttl": "8760h",
  "allow_any_name": "true"
}
EOT
}
</code></pre>

<p>If you add that with your ECS task along with the vault role and policy from the previous post about Vault Anything in Terraform, all your dependencies are met and kept in code!</p>

<p>You can now generate certificates off your PKI backend and grab secrets all in one go!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vault as a CA for ECS containers using Terraform (Part 1)]]></title>
    <link href="https://marcyoung.us/post/docker-vault-ca-part1"/>
    <updated>2017-12-24T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/docker-vault-ca-part1</id>
    <content type="html"><![CDATA[<p>In this I&rsquo;ll show some examples on how to leverage HashiCorp Vault as an awesome CA. Note this part is boring and 80% copy/paste from a cuddletech blog. <!-- more --></p>

<h2>Prerequisites</h2>

<ol>
<li>Vault running as a server</li>
<li>Vault CLI to match the server</li>
</ol>


<h2>Create the Root cert/key</h2>

<p>(This is actually mostly a clone from <a href="https://cuddletech.com/?p=959">here</a> but I want to make sure it never disappears)</p>

<p><em>Note</em>: You should not let the vault server generate the root CA, you should generate it and import it. I just havent figured out how to do that yet. I will update this post when I get around to it. <a href="https://en.wikipedia.org/wiki/Offline_root_certificate_authority">More info on that here</a></p>

<ol>
<li>Mount the PKI backend for the root</li>
</ol>


<pre><code>$ vault mount -path=cuddletech -description="Cuddletech Root CA" -max-lease-ttl=87600h pki
Successfully mounted 'pki' at 'cuddletech'!
$ vault mounts
Path         Type       Default TTL  Max TTL    Description
cubbyhole/   cubbyhole  n/a          n/a        per-token private secret storage
cuddletech/  pki        system       315360000  Cuddletech Root CA
secret/      generic    system       system     generic secret storage
sys/         system     n/a          n/a        system endpoints used for control, policy and debugging 
</code></pre>

<pre><code>$ vault write cuddletech/root/generate/internal \
&gt; common_name="Cuddletech Root CA" \
&gt; ttl=87600h \
&gt; key_bits=4096 \
&gt; exclude_cn_from_sans=true
Key             Value
---             -----
certificate     -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
..
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
expiration      1783411981
issuing_ca      -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
...
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
serial_number   0d:78:88:dc:60:f3:3f:62:1b:43:d2:1a:b4:54:82:bf:d3:ea:fe:58
</code></pre>

<p>Verify</p>

<pre><code>$ curl -s https://localhost:8200/v1/cuddletech/ca/pem | openssl x509 -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            0d:78:88:dc:60:f3:3f:62:1b:43:d2:1a:b4:54:82:bf:d3:ea:fe:58
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Cuddletech Root CA
        Validity
            Not Before: Jul  9 08:12:31 2016 GMT
            Not After : Jul  7 08:13:01 2026 GMT
        Subject: CN=Cuddletech Root CA
...
</code></pre>

<p>Fix the URL for accessing the CA/CRL URLs</p>

<pre><code>$ vault write cuddletech/config/urls issuing_certificates="https://10.0.0.22:8200/v1/cuddletech
Success! Data written to: cuddletech/config/urls
</code></pre>

<h2>Create the Intermediate cert/key</h2>

<p>Mount the PKI for the Intermediate (similar to root)</p>

<pre><code>vault mount -path=cuddletech_ops -description="Cuddletech Ops Intermediate CA" -max-lease-ttl=26280h pki
Successfully mounted 'pki' at 'cuddletech_ops'!

$ vault mounts
Path             Type       Default TTL  Max TTL    Description
cubbyhole/       cubbyhole  n/a          n/a        per-token private secret storage
cuddletech/      pki        system       315360000  Cuddletech Root CA
cuddletech_ops/  pki        system       94608000   Cuddletech Ops Intermediate CA
</code></pre>

<p>Generate the intermediate CSR</p>

<pre><code>$ vault write cuddletech_ops/intermediate/generate/internal \
&gt;  common_name="Cuddletech Operations Intermediate CA"
&gt;  ttl=26280h \
&gt;  key_bits=4096 \
&gt;  exclude_cn_from_sans=true
Key     Value
---     -----
csr     -----BEGIN CERTIFICATE REQUEST-----
MIICuDCCAaACAQAwMDEuMCwGA1UEAxMlQ3VkZGxldGVjaCBPcGVyYXRpb25zIElu
dGVybWVkaWF0ZSBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALt8
...
hD8cpHTXqjKExYWKc/rQDgjw9+RNDdb45xszDagrgFgNPqI9i0fNh9jViMmjUiTc
PQTZS4XxIoRrx1/xVHJ4Qm++ntLPVCvzjMZafg==
-----END CERTIFICATE REQUEST-----
</code></pre>

<p>Cut and paste that CSR into a new file cuddletech_ops.csr. The reason we output the file here is so we can get it out of one backend and into another and then back out.</p>

<pre><code>$ vault write cuddletech/root/sign-intermediate \
&gt;  csr=@cuddletech_ops.csr \
&gt;  common_name="Cuddletech Ops Intermediate CA" \
&gt;  ttl=8760h
Key             Value
---             -----
certificate     -----BEGIN CERTIFICATE-----
MIIEZDCCAkygAwIBAgIUHuIhRF3tYtfoZiAFdjcCtQpMR+cwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4Mjkz
...
UtI2b/AamAqf340eRKmSdEh4WypB4JR+t259YA45w2j4mS+rxREycEk4YosR/vUs
jekMiq57yNq7h8eOTrnOulJxazbVrYGb
-----END CERTIFICATE-----
expiration      1470645002
issuing_ca      -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
..
1FRGlwHUg+6IIZBVIapzivLc6pAvLFPxQlQvT5CNHPk91zwyNQ9ZX2PzatdajUnd
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
serial_number   1e:e2:21:44:5d:ed:62:d7:e8:66:20:05:76:37:02:b5:0a:4c:47:e7
</code></pre>

<p>Now that we have a Root CA signed cert, we’ll need to cut-n-paste this certificate into a file we’ll name cuddletech_ops.crt and then import it into our Intermediate CA backend:</p>

<pre><code>$ vault write cuddletech_ops/intermediate/set-signed \
&gt; certificate=@cuddletech_ops.crt
Success! Data written to: cuddletech_ops/intermediate/set-signed
</code></pre>

<p>Awesome! Lets verify:</p>

<pre><code>$ curl -s https://localhost:8200/v1/cuddletech_ops/ca/pem | openssl x509 -text | head -20
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            76:12:53:41:be:18:98:2c:a1:51:4a:f8:f0:bd:b4:a3:44:7e:74:59
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Cuddletech Root CA
        Validity
            Not Before: Jul  9 09:23:39 2016 GMT
            Not After : Jul  9 09:24:09 2017 GMT
        Subject: CN=Cuddletech Ops Intermediate CA
 ...
</code></pre>

<p>The last thing we need to do is set the CA &amp; CRL URL’s for accessing the CA:</p>

<pre><code>$ vault write cuddletech_ops/config/urls \
&gt; issuing_certificates="https://10.0.0.22:8200/v1/cuddletech_ops/ca" \
&gt; crl_distribution_points="https://10.0.0.22:8200/v1/cuddletech_ops/crl"
Success! Data written to: cuddletech_ops/config/urls
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS ECS Hot/Warm ELK + Curator Part 4 - Curator]]></title>
    <link href="https://marcyoung.us/post/dockerized-elk-part-4"/>
    <updated>2016-08-08T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/dockerized-elk-part4</id>
    <content type="html"><![CDATA[<p>The previous part focused on getting Kibana up. This one focuses on getting Curator <!-- more --></p>

<h1>Disclaimer</h1>

<p>I&rsquo;m redacting any information that might seem sensitive such as account numbers. Use your discretion and make sure you use values that make sense for things blacked out in images or in <code>{}</code> notation.</p>

<h1>Part 2 - Curator</h1>

<p>This is the coolest part of the whole stack.</p>

<p>We&rsquo;re basically going to build/deploy a docker image for Curator, then upload a cloudformation template that creates a Lambda function to run it.</p>

<p>The lambda function will have a trigger of your choice (probably a scheduled event for once a day if I had to guess) and will run two tasks.</p>

<p>Task 1 is a rotate warm task that will tell Elasticsearch to move any indexes to warm that (in this case) are 0 days old (for demo purposes).
Task 2 is a delete task that tells Elasticsearch to delete any indexes older than 14 days.</p>

<p>You can expand this to stagger them, take snapshots, etc. This allows you to have schedules that define how the data moves from box to box or to backups!</p>

<p>First thing we need to do is build the container and push it to ECR.</p>

<p>In the ECR portion of the AWS console, create a new repository called <code>test/curator</code>.</p>

<p>Then in your console for <a href="https://github.com/myoung34/elk-docker-aws/blob/master/curator/Dockerfile">the curator dockerfile</a> run:</p>

<pre><code>docker build -t curator:local .
docker tag curator:local \
   {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/curator:latest
$(aws ecr get-login)
docker push {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/curator:latest
</code></pre>

<p>Once this is pushed you can verify it by looking for a tag <code>latest</code> in your <code>test/curator</code> ECR repository.</p>

<p>Next upload <a href="https://github.com/myoung34/elk-docker-aws/blob/master/curator/cloudformation.json">this cloudformation template</a> to Cloudformation (modifying the parameters as you need).</p>

<p><img src="../../images/elk/curator_cft_params.png" alt="" /></p>

<p>Once it is complete, go to the Lambda portion of AWS and click your function that was created. You can test it by clicking &ldquo;Test&rdquo; and just hitting &ldquo;submit&rdquo;.</p>

<p>You will see some output such as:</p>

<p><img src="../../images/elk/curator_lambda.png" alt="" /></p>

<p>If you go to the ECS console quickly you will see two tasks have been created and are being run for the first time (will be in pending for a minute while the image is being pulled to the instance running it).</p>

<p><img src="../../images/elk/curator_delete_task.png" alt="" /></p>

<p><img src="../../images/elk/curator_rotate_warm_task.png" alt="" /></p>

<p>And now if you look at your kopf plugin you will see the data move from the &ldquo;hot&rdquo; node:</p>

<p><img src="../../images/elk/curator_before.png" alt="" /></p>

<p>To eventually the &ldquo;warm&rdquo; node:</p>

<p><img src="../../images/elk/curator_after.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS ECS Hot/Warm ELK + Curator Part 3 - Kibana]]></title>
    <link href="https://marcyoung.us/post/dockerized-elk-part-3"/>
    <updated>2016-08-08T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/dockerized-elk-part3</id>
    <content type="html"><![CDATA[<p>The previous part focused on getting logstash up. This one focuses on getting Kibana <!-- more --></p>

<h1>Disclaimer</h1>

<p>I&rsquo;m redacting any information that might seem sensitive such as account numbers. Use your discretion and make sure you use values that make sense for things blacked out in images or in <code>{}</code> notation.</p>

<h1>Part 2 - Kibana</h1>

<p>First thing we need to do is build the container and push it to ECR.</p>

<p>In the ECR portion of the AWS console, create a new repository called <code>test/kibana</code>.</p>

<p>Then in your console for <a href="https://github.com/myoung34/elk-docker-aws/blob/master/kibana/Dockerfile">the kibana dockerfile</a> run:</p>

<pre><code>docker build -t kibana:local .
docker tag kibana:local \
   {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/kibana:latest
$(aws ecr get-login)
docker push {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/kibana:latest
</code></pre>

<p>Once this is pushed you can verify it by looking for a tag <code>latest</code> in your <code>test/kibana</code> ECR repository.</p>

<p>Next upload <a href="https://github.com/myoung34/elk-docker-aws/blob/master/kibana/cloudformation.json">this cloudformation template</a> to Cloudformation (modifying the parameters as you need).</p>

<p>It will look almost exactly like the logstash upload in terms of parameters.</p>

<p>You now have a kibana instance! My next step would be to configure a Route53 domain name to point to the kibana load balancer so you can have real SSL without any chain issues.</p>

<h1>Takeaways</h1>

<p>Similar to what we did with elasticsearch and logstash, kibana is listening on port <code>5601</code> via HTTPS. It uses self-signed SSL certificates. The dockerfile is actually identical to the verified one on the Dockerhub, except I expanded the <code>docker-entrypoint.sh</code> to take more parameters since the base one doesn&rsquo;t allow much configurability: <a href="https://github.com/docker-library/kibana/pull/45">https://github.com/docker-library/kibana/pull/45</a></p>

<p>If you browse to your ELB or route53 entry on port 443, you&rsquo;ll be greeted by kibana.</p>

<p><img src="../../images/elk/kibana_first.png" alt="" /></p>

<p>If you configure your index to <code>test-*</code> your dashboard will show the logs Logstash ingested!</p>

<p><img src="../../images/elk/kibana_dash.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
