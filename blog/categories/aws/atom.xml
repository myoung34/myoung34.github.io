<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: aws | Marcus Young]]></title>
  <link href="http://marcyoung.us/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://marcyoung.us/"/>
  <updated>2017-12-22T13:32:04-06:00</updated>
  <id>http://marcyoung.us/</id>
  <author>
    <name><![CDATA[Marcus Young]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Up and Running With Bless and Enforced MFA - Part 2]]></title>
    <link href="http://marcyoung.us/post/bless-part2"/>
    <updated>2017-06-20T00:00:00-05:00</updated>
    <id>http://marcyoung.us/post/bless-part2</id>
    <content type="html"><![CDATA[<p>Part 2 of deploying Bless will focus on enforcing MFA and using Lyft’s client. <!-- more --></p>

<p>The problem at this point is that anyone in the ops group can bounce without MFA enforced. We can set up the lyft client, which enforces MFA, but if we dont make other changes, there’s no enforcement of MFA. ie you can still call the netflix client and bounce right in.</p>

<p>The other issue is that Netflix’s bless holds true the idea of a bastion host. In-house, we have IPSec tunnels from our VPC’s to our internal network, so I dont need to bounce through a bastion. So I want anyone to be able to bounce, given that they have permissions (ops) and they used MFA. Lyft’s doesn’t enforce the bastion concept, which is nice.</p>

<h1>Prerequisites</h1>

<p>In the IAM console in AWS, enable an MFA device for your user.</p>

<h2>Setting up the client</h2>

<p>I wrote a wrapper to simplify this. Feel free to not like it, but I put this in my <code>~/.bash_aliases</code> to make it easier:</p>

<pre><code>function bounce() {
  if [[ ! -d ~/.blessclient ]]; then
    pushd . &gt;/dev/null
    git clone https://github.com/lyft/python-blessclient.git ~/.blessclient
    cd $_
    make
    popd &gt;/dev/null
  fi
  INTERNAL_IP=$(ifconfig | grep inet | grep 192 | awk '{ print $2}')
  # This gets weird if you have wired and wifi hooked up (sometimes i do). It might allow the wrong IP and give you a crap error messsage.
  if [[ $(ifconfig | grep inet | grep 192 | awk '{ print $2}' | wc -l) -gt 1 ]] &amp;&amp; [[ -z ${BLESSFIXEDIP} ]]; then
    echo "More than one internal ip found. Disable a network interface or provide it manually via env var 'BLESSFIXEDIP'"
  else
    FILE="$(mktemp)"
    rm -rf ${FILE}*
    ssh-keygen -f ${FILE} -N ""
    BLESS_IDENTITYFILE=${FILE} BLESSFIXEDIP=${BLESSFIXEDIP:-${INTERNAL_IP}} ~/.blessclient/blessclient.run --host $1 --nocache --config ~/.blessclient/blessclient.cfg --region EAST &amp;&amp; ssh -i ${FILE} ec2-user@$1
  fi
}
</code></pre>

<p>Now you can bounce with:</p>

<pre><code>$ bounce 10.0.3.7
Generating public/private rsa key pair.
Your identification has been saved in /var/folders/5j/n9trdvbn3qqdtkshkpvnrkhh0000gp/T/tmp.s2ZdeuJD.
Your public key has been saved in /var/folders/5j/n9trdvbn3qqdtkshkpvnrkhh0000gp/T/tmp.s2ZdeuJD.pub.
The key fingerprint is:
SHA256:CizQBI/ROY8QIeiRjt7rY65jPc/oJDL3MA marcyoung@Admins-MacBook-Pro.local
The key's randomart image is:
+---[RSA 2048]----+
|B*+.             |
|++o              |
|=..              |
|.o.*.            |
|. o  S           |
|...+ . .         |
| E .o.           |
|oo= + .          |
|.=*              |
+----[SHA256]-----+
Enter your AWS MFA code: 477821
Requesting certificate for your public key (set BLESSQUIET=1 to suppress these messages)
Finished getting certificate.
Last login: Fri Jun 16 20:14:52 2017 from ip-192-168-1-124.ec2.internal

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2017.03-release-notes/
No packages needed for security; 2 packages available
Run "sudo yum update" to apply all updates.
[marc@server ~]$
</code></pre>

<p>Nice! But again, MFA is used, not enforced. Lets change that.</p>

<p>I hinted at my group policy in part 1 for <code>ops</code> that looked like:</p>

<pre><code>resource "aws_iam_group_policy" "bless-client" {
    name = "bless-client_iam_policy"
    group = "${aws_iam_group.ops.id}"
    policy = &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Action": "sts:AssumeRole",
        "Resource": [
          "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
        ]
      },
      {
        "Action": "kms:Encrypt",
        "Effect": "Allow",
        "Resource": [
          "${data.terraform_remote_state.kms.ops_bless_arn}"
        ],
        "Condition": {
          "StringEquals": {
            "kms:EncryptionContext:user_type": "user",
            "kms:EncryptionContext:from": "$${aws:username}"
          }
        }
      }
    ]
}
EOF
}
</code></pre>

<p>But in actuality, it should look more like:</p>

<pre><code>resource "aws_iam_group_policy" "bless-client" {
    name = "assume-bless-role"
    group = "${aws_iam_group.ops.id}"
    policy = &lt;&lt;EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": [
        "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
      ],
      "Condition": {
        "Bool": {
          "aws:MultiFactorAuthPresent": "true"
        }
      }
    },
    {
      "Effect": "Deny",
      "Action": "sts:AssumeRole",
      "Resource": [
        "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
      ],
      "Condition": {"BoolIfExists": {"aws:MultiFactorAuthPresent": false}}
    },
    {
      "Action": "kms:Encrypt",
      "Effect": "Allow",
      "Resource": [
        "${data.terraform_remote_state.kms.ops_bless_arn}"
      ],
      "Condition": {
        "StringEquals": {
          "kms:EncryptionContext:user_type": "user",
          "kms:EncryptionContext:from": "$${aws:username}"
        },
        "Bool": {
          "aws:MultiFactorAuthPresent": "true"
        }
      }
    },
    {
      "Action": "kms:Encrypt",
      "Effect": "Deny",
      "Resource": [
        "${data.terraform_remote_state.kms.ops_bless_arn}"
      ],
      "Condition": {
        "StringEquals": {
          "kms:EncryptionContext:user_type": "user",
          "kms:EncryptionContext:from": "$${aws:username}"
        },
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": false
        }
      }
    }
  ]
}
EOF
}
</code></pre>

<p>What did we do? We basically said ops users can assume the bless-client role or call kms:Encrypt on that key, but only if they used MFA. If you try the original bless client now, it will fail.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Up and Running With Bless and Enforced MFA - Part 1]]></title>
    <link href="http://marcyoung.us/post/bless-part1"/>
    <updated>2017-06-20T00:00:00-05:00</updated>
    <id>http://marcyoung.us/post/bless-part1</id>
    <content type="html"><![CDATA[<p>I finally got Netflix’s <a href="http://github.com/netflix/bless">Bless</a> running in production using a forked version of Lyft’s <a href="https://github.com/lyft/python-blessclient">client</a>. This post will focus on the first and easier portion: Bless in Lambda <!-- more --></p>

<h1>Prerequisites</h1>

<p>Time and terraform. I like terraform because I dislike cloudformation. Feel free to adapt.</p>

<h2>Installing Bless</h2>

<p>This is actually the easiest part, because it’s straight forward. Side note: I like bless because of its simplicity. It uses lambda + KMS and nothing more. The permissions are stupid simple. The purpose, after talking to one of the guys behind it, was to make it feel native and un-intrusive. It works.</p>

<h2>Setup Config</h2>

<p>This was the hardest to get going, because configuration always is. The readme says to paste in a function to lambda to generate your password. But thats not needed with the super awesome lambda container from <a href="https://github.com/lambci/docker-lambda">lambci</a>. To do this, create a file function.py with the contents:</p>

<pre><code>import boto3
import base64
import os


def lambda_handler(event, context):
    region = os.environ['AWS_REGION']
    client = boto3.client('kms', region_name=region)
    response = client.encrypt(
    KeyId='kms_key_id',
    Plaintext='totallysecure'
    )

    ciphertext = response['CiphertextBlob']
    return base64.b64encode(ciphertext)
</code></pre>

<p>Now lets run that without using the real lambda. Make sure your access key/id have the required permissions to do <code>kms:Encrypt</code>:</p>

<pre><code>docker run -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY=AWS_SECRET_ACCESS_KEY -e AWS_REGION=us-east-1 -v "$PWD":/var/task lambci/lambda:python2.7 function.lambda_handler
START RequestId: 42f85048-91e1-4b48-9376-0563722616f0 Version: $LATEST
END RequestId: 42f85048-91e1-4b48-9376-0563722616f0
REPORT RequestId: 42f85048-91e1-4b48-9376-0563722616f0 Duration: 733 ms Billed Duration: 800 ms Memory Size: 1536 MB Max Memory Used: 25 MB
"some base64 encoded key"
</code></pre>

<h2>The Config</h2>

<p>This is an example of a finished config. Save it as <code>lambda_configs/bless_deploy.cfg</code>:</p>

<pre><code>[Bless Options]
certificate_validity_after_seconds = 120
certificate_validity_before_seconds = 120
entropy_minimum_bits = 2048
random_seed_bytes = 256
logging_level = INFO

[Bless CA]

us-east-1_password = some base64 encoded key from lambci/lambda
ca_private_key_file = foo1.pem

[KMS Auth]
</code></pre>

<h2>Build</h2>

<pre><code>$ virtualenv venv
$ . venv/bin/activate
$ pip install -r requirements.txt
$ make publish 
</code></pre>

<h1>Deploy</h1>

<h2>IAM</h2>

<p>The first piece is the IAM portion for Bless. I need to allow it to:</p>

<ol>
<li>Generate random keys from KMS (obvious)</li>
<li>Decrypt the KMS key that’s the password for Bless. This is generated and</li>
<li>Create the log group (maybe not necesary since Lambda does that for you?)</li>
<li>Push logs</li>
</ol>


<pre><code>resource "aws_iam_role" "bless_lambda" {
    name = "bless_lambda"
    assume_role_policy = &lt;&lt;EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Effect": "Allow",
      "Sid": ""
    }
  ]
}
EOF
}

resource "aws_iam_role_policy" "bless_lambda" {
    name = "bless_lambda"
    role = "${aws_iam_role.bless_lambda.id}"
    policy = &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
          "Sid": "Stmt1443036478000",
          "Effect": "Allow",
          "Action": [
              "kms:GenerateRandom",
              "kms:Decrypt"
          ],
          "Resource": [
              "${data.terraform_remote_state.kms.ops_bless_arn}"
          ]
        },
        {
            "Effect": "Allow",
            "Action": "logs:CreateLogGroup",
            "Resource": "arn:aws:logs:us-east-1:*:*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": [
                "arn:aws:logs:us-east-1:*:log-group:/aws/lambda/*"
            ]
        }
    ]
}
EOF
}
</code></pre>

<h2>KMS</h2>

<pre><code>resource "aws_kms_key" "ops-bless" {
    description             = "KMS key for Bless"
    deletion_window_in_days = 7
}

resource "aws_kms_alias" "ops-bless" {
    name          = "alias/ops/bless"
    target_key_id = "${aws_kms_key.ops-bless.key_id}"
}

output "ops_bless_arn" {
  value = "${aws_kms_key.ops-bless.arn}"
}
</code></pre>

<h1>Client</h1>

<h2>Client IAM role to assume</h2>

<p>It basically just allows the invocation of our deployed function:</p>

<pre><code>resource "aws_iam_role" "bless-client" {
    name               = "bless-client"
    path               = "/"
    assume_role_policy = &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": "lambda:InvokeFunction",
            "Effect": "Allow",
            "Resource": [
                "${data.terraform_remote_state.lambda-bless.bless_arn}"
            ]
        }
    ]
}
EOF
}
</code></pre>

<p>I added this to our <code>ops</code> IAM group so that any ops person can assume the role or use <code>kms:Encrypt</code> from the bless KMS key:</p>

<pre><code>resource "aws_iam_group_policy" "bless-client" {
    name = "bless-client_iam_policy"
    group = "${aws_iam_group.ops.id}"
    policy = &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Action": "sts:AssumeRole",
        "Resource": [
          "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
        ]
      },
      {
        "Action": "kms:Encrypt",
        "Effect": "Allow",
        "Resource": [
          "${data.terraform_remote_state.kms.ops_bless_arn}"
        ],
        "Condition": {
          "StringEquals": {
            "kms:EncryptionContext:user_type": "user",
            "kms:EncryptionContext:from": "$${aws:username}"
          }
        }
      }
    ]
}
EOF
}
</code></pre>

<h2>Client config</h2>

<p>This is an example of what my client config looks like:</p>

<pre><code>[MAIN]
region_aliases: east
kms_service_name: bless
bastion_ips: 192.168.1.6
remote_user: ec2-user

[CLIENT]
domain_regex: (.*\.mycompany\.com|.*\.example\.net|\A10\.[0-9](?:\.[0-9]{1,3}){2}\Z)$
cache_dir: .bless/session
cache_file: bless_cache.json
mfa_cache_dir: .aws/session
mfa_cache_file: token_cache.json
ip_urls: http://api.ipify.org, http://canihazip.com
update_script: update_blessclient.sh

[LAMBDA]
user_role: bless-client
account_id: my account id
functionname: bless
functionversion: $LATEST
certlifetime: 120
ipcachelifetime: 120
timeout_connect: 30
timeout_read: 30

[REGION_EAST]
awsregion: us-east-1
kmsauthkey: my kms id that i used for the original password encrypt
</code></pre>

<h1>Test</h1>

<p>You should now have a lambda that can generate certificates to use. You’ll need to put the public keys from <code>foo1.pub</code> and <code>foo2.pub</code> into your servers at <code>/etc/ssh/cas.pub</code> and add <code>TrustedUserCAKeys /etc/ssh/cas.pub</code> to <code>/etc/ssh/sshd_config</code> You can test this with their client:</p>

<pre><code>function bounce () {
  FILE="$(mktemp)"
  rm -rf ${FILE}
  ssh-keygen -f ${FILE} -N ""
  ./bless_client.py \
    us-east-1 \
    bless \
    testdev $(curl --silent http://ipecho.net/plain) \
    ec2-user $(ifconfig | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*' | grep -Eo '([0-9]*\.){3}[0-9]*' | grep -v '127.0.0.1') \
    "" \
    ${FILE}.pub ${FILE}-cert.pub
  ssh -i ${FILE} ec2-user@$1 "${@:2}"
}

$ bounce 10.0.0.19
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS ECS Hot/Warm ELK + Curator Part 4 - Curator]]></title>
    <link href="http://marcyoung.us/post/dockerized-elk-part-4"/>
    <updated>2016-08-08T00:00:00-05:00</updated>
    <id>http://marcyoung.us/post/dockerized-elk-part4</id>
    <content type="html"><![CDATA[<p>The previous part focused on getting Kibana up. This one focuses on getting Curator <!-- more --></p>

<h1>Disclaimer</h1>

<p>I&rsquo;m redacting any information that might seem sensitive such as account numbers. Use your discretion and make sure you use values that make sense for things blacked out in images or in <code>{}</code> notation.</p>

<h1>Part 2 - Curator</h1>

<p>This is the coolest part of the whole stack.</p>

<p>We&rsquo;re basically going to build/deploy a docker image for Curator, then upload a cloudformation template that creates a Lambda function to run it.</p>

<p>The lambda function will have a trigger of your choice (probably a scheduled event for once a day if I had to guess) and will run two tasks.</p>

<p>Task 1 is a rotate warm task that will tell Elasticsearch to move any indexes to warm that (in this case) are 0 days old (for demo purposes).
Task 2 is a delete task that tells Elasticsearch to delete any indexes older than 14 days.</p>

<p>You can expand this to stagger them, take snapshots, etc. This allows you to have schedules that define how the data moves from box to box or to backups!</p>

<p>First thing we need to do is build the container and push it to ECR.</p>

<p>In the ECR portion of the AWS console, create a new repository called <code>test/curator</code>.</p>

<p>Then in your console for <a href="https://github.com/myoung34/elk-docker-aws/blob/master/curator/Dockerfile">the curator dockerfile</a> run:</p>

<pre><code>docker build -t curator:local .
docker tag curator:local \
   {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/curator:latest
$(aws ecr get-login)
docker push {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/curator:latest
</code></pre>

<p>Once this is pushed you can verify it by looking for a tag <code>latest</code> in your <code>test/curator</code> ECR repository.</p>

<p>Next upload <a href="https://github.com/myoung34/elk-docker-aws/blob/master/curator/cloudformation.json">this cloudformation template</a> to Cloudformation (modifying the parameters as you need).</p>

<p><img src="../../images/elk/curator_cft_params.png" alt="" /></p>

<p>Once it is complete, go to the Lambda portion of AWS and click your function that was created. You can test it by clicking &ldquo;Test&rdquo; and just hitting &ldquo;submit&rdquo;.</p>

<p>You will see some output such as:</p>

<p><img src="../../images/elk/curator_lambda.png" alt="" /></p>

<p>If you go to the ECS console quickly you will see two tasks have been created and are being run for the first time (will be in pending for a minute while the image is being pulled to the instance running it).</p>

<p><img src="../../images/elk/curator_delete_task.png" alt="" /></p>

<p><img src="../../images/elk/curator_rotate_warm_task.png" alt="" /></p>

<p>And now if you look at your kopf plugin you will see the data move from the &ldquo;hot&rdquo; node:</p>

<p><img src="../../images/elk/curator_before.png" alt="" /></p>

<p>To eventually the &ldquo;warm&rdquo; node:</p>

<p><img src="../../images/elk/curator_after.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS ECS Hot/Warm ELK + Curator Part 3 - Kibana]]></title>
    <link href="http://marcyoung.us/post/dockerized-elk-part-3"/>
    <updated>2016-08-08T00:00:00-05:00</updated>
    <id>http://marcyoung.us/post/dockerized-elk-part3</id>
    <content type="html"><![CDATA[<p>The previous part focused on getting logstash up. This one focuses on getting Kibana <!-- more --></p>

<h1>Disclaimer</h1>

<p>I&rsquo;m redacting any information that might seem sensitive such as account numbers. Use your discretion and make sure you use values that make sense for things blacked out in images or in <code>{}</code> notation.</p>

<h1>Part 2 - Kibana</h1>

<p>First thing we need to do is build the container and push it to ECR.</p>

<p>In the ECR portion of the AWS console, create a new repository called <code>test/kibana</code>.</p>

<p>Then in your console for <a href="https://github.com/myoung34/elk-docker-aws/blob/master/kibana/Dockerfile">the kibana dockerfile</a> run:</p>

<pre><code>docker build -t kibana:local .
docker tag kibana:local \
   {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/kibana:latest
$(aws ecr get-login)
docker push {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/kibana:latest
</code></pre>

<p>Once this is pushed you can verify it by looking for a tag <code>latest</code> in your <code>test/kibana</code> ECR repository.</p>

<p>Next upload <a href="https://github.com/myoung34/elk-docker-aws/blob/master/kibana/cloudformation.json">this cloudformation template</a> to Cloudformation (modifying the parameters as you need).</p>

<p>It will look almost exactly like the logstash upload in terms of parameters.</p>

<p>You now have a kibana instance! My next step would be to configure a Route53 domain name to point to the kibana load balancer so you can have real SSL without any chain issues.</p>

<h1>Takeaways</h1>

<p>Similar to what we did with elasticsearch and logstash, kibana is listening on port <code>5601</code> via HTTPS. It uses self-signed SSL certificates. The dockerfile is actually identical to the verified one on the Dockerhub, except I expanded the <code>docker-entrypoint.sh</code> to take more parameters since the base one doesn&rsquo;t allow much configurability: <a href="https://github.com/docker-library/kibana/pull/45">https://github.com/docker-library/kibana/pull/45</a></p>

<p>If you browse to your ELB or route53 entry on port 443, you&rsquo;ll be greeted by kibana.</p>

<p><img src="../../images/elk/kibana_first.png" alt="" /></p>

<p>If you configure your index to <code>test-*</code> your dashboard will show the logs Logstash ingested!</p>

<p><img src="../../images/elk/kibana_dash.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS ECS Hot/Warm ELK + Curator Part 2 - Logstash]]></title>
    <link href="http://marcyoung.us/post/dockerized-elk-part-2"/>
    <updated>2016-08-08T00:00:00-05:00</updated>
    <id>http://marcyoung.us/post/dockerized-elk-part2</id>
    <content type="html"><![CDATA[<p>The previous part focused on getting the ECS cluster, ECR repos, and Elasticsearch up. This one focuses on getting Logstash <!-- more --></p>

<h1>Disclaimer</h1>

<p>I&rsquo;m redacting any information that might seem sensitive such as account numbers. Use your discretion and make sure you use values that make sense for things blacked out in images or in <code>{}</code> notation.</p>

<h1>Part 2a - Logstash</h1>

<p>First thing we need to do is build the container and push it to ECR.</p>

<p>In the ECR portion of the AWS console, create a new repository called <code>test/logstash</code>.</p>

<p>Then in your console for <a href="https://github.com/myoung34/elk-docker-aws/blob/master/logstash/Dockerfile">the logstash dockerfile</a> run:</p>

<pre><code>docker build -t logstash:local .
docker tag logstash:local \
   {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/logstash:latest
$(aws ecr get-login)
docker push {acctnum}.dkr.ecr.us-east-1.amazonaws.com/test/logstash:latest
</code></pre>

<p>Once this is pushed you can verify it by looking for a tag <code>latest</code> in your <code>test/logstash</code> ECR repository.</p>

<p>Next upload <a href="https://github.com/myoung34/elk-docker-aws/blob/master/logstash/cloudformation.json">this cloudformation template</a> to Cloudformation (modifying the parameters as you need).</p>

<p><img src="../../images/elk/logstash_cft.png" alt="" /></p>

<p>You now have a listening logstash instance! My next step would be to configure a Route53 domain name to point to the logstash load balancer so you can have real SSL without any chain issues.</p>

<h1>Takeaways</h1>

<p>Similar to what we did with elasticsearch, logstash is listening on port <code>5000</code> for beats input. It uses self-signed SSL certificates. Most beats forwarders won&rsquo;t play nice with that out of the box, but if you configure Route53 to point to the ELB and use ACM, it will be valid SSL. The load balancer will use TCP to send to the logstash instances and not care that it is self-signed.</p>

<p>The <a href="https://github.com/myoung34/elk-docker-aws/blob/master/logstash/logstash.conf">logstash configuration file</a> uses dynamic indexes, so whatever you set <code>document_type</code> on your beats configuration to will become the index.</p>

<h1>Part 2b - Send to logstash</h1>

<p>Install the latest stable logstash and use this filebeat configuration to get ready to ship:</p>

<pre><code>filebeat:
  prospectors:
    -
      paths:
        - "/var/log/test/*"
      encoding: plain
      input_type: log
      fields_under_root: true
      document_type: test
output:
  logstash:
    hosts: ["log-test.dev.mydom.com:5000"]
</code></pre>

<p>Next make sure that directory exists via <code>sudo mkdir /var/log/test</code> and start the service (depends on your OS): <code>sudo service filebeat start</code>.</p>

<p>Lastly, lets send something to logstash: <code>echo asdf | sudo tee -a /var/log/test/$(uuidgen)</code>. That will generate a random file and put <code>asdf</code> into it. It should show up in your kopf plugin to view:</p>

<p><img src="../../images/elk/logstash_kopf.png" alt="" /></p>
]]></content>
  </entry>
  
</feed>
