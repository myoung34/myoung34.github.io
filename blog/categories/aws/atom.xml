<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

  <title><![CDATA[Category: aws | Marcus Young]]></title>
  <link href="https://marcyoung.us/blog/categories/aws/atom.xml" rel="self"/>
  <link href="https://marcyoung.us/"/>
  <updated>2023-04-26T02:42:46+00:00</updated>
  <id>https://marcyoung.us/</id>
  <author>
    <name><![CDATA[Marcus Young]]></name>
    
  </author>
  <generator uri="https://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Just In Time SSH Using EC2 Instance Connect]]></title>
    <link href="https://marcyoung.us/post/ec2-instance-connect-ssh"/>
    <updated>2023-04-25T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/ec2-instance-connect-ssh</id>
    <content type="html"><![CDATA[<p>I played around with just in time users in EC2 using instance connect and a custom built NSS module <!-- more --></p>

<p>I needed to PoC a solution for SSH users that are backed by an IdP but ran into some issues.</p>

<p>EC2 Instance Connect (IMO) seems to be way under promoted by AWS, and I feel like that&rsquo;s a real disservice if you need (I&rsquo;m so sorry for you) SSH (I do for this case).</p>

<p>The TLDR for ec2 instance connect:</p>

<ul>
<li>A user makes a call <code>aws ec2-instance-connect send-ssh-public-key --instance-id i-08e2c277f1ea9a99a --instance-os-user marcus.young --ssh-public-key file:///home/myoung/.ssh/asdf.pub</code></li>
<li>AWS pushes the public key to <code>http://169.254.169.254/latest/meta-data/managed-ssh-keys/active-keys/marcus.young</code> for <code>60</code> seconds</li>
<li>The user does <code>ssh -i {private key that matches the public key} marcus.young@{public ip of the instance}</code> and gets in.</li>
</ul>


<p>That&rsquo;s crazy cool, but we have a few issues outright (major ones):</p>

<ul>
<li>You can impersonate. There&rsquo;s nothing keeping me from doing <code>--instance-os-user not.me</code> and getting in. Fine in small cases bad for observability. You could tie this back to a user but that sucks. Youd have to tie back the <code>SendSSHPublicKey</code> cloudtrail call with the iam user/role via the request params to the box, and if youre not shipping auth.log youre screwed. If 100 people do this at once you&rsquo;ve lost. There&rsquo;s no way to see which key material let who in to run what. If you thought &ldquo;Oh I&rsquo;ll use sts principaltags&rdquo; I love you. Keep reading.</li>
<li>Users have to exist ahead of time. This is the <strong>big</strong> one. If you thought &ldquo;What if we create them when they send they create the key on the host&rdquo;. You can&rsquo;t. Not easily anyway. If you think it&rsquo;s still solvable or said &ldquo;What about NSS&rdquo; keep reading homie.</li>
<li>There&rsquo;s no documented rate limits. I&rsquo;ve brought this up, no answer yet. I might see if I can break this ;)</li>
<li>You cannot <em>directly</em> do this to private instances. You&rsquo;ll need public instances or bastions to do this. I&rsquo;ll give you a snippet for that too.</li>
</ul>


<p>Let&rsquo;s get started.</p>

<p>First we need to bring up an Okta with IAM identity center.
Sorry peeps but if you think I&rsquo;m doing <strong>that</strong> walkthrough just <code>ctrl+w</code> and touch grass. Not happening.</p>

<p>Let&rsquo;s assume you have a working IdP and you can get into AWS. Let&rsquo;s start there.</p>

<p>So impersonation&hellip;.</p>

<p>The answer here is an SCP and <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_session-tags.html">sts principalTags</a>.
For the principal tags, it&rsquo;s sort of straight forward. Unless you discover an IAM identity center bug like I did.
If you&rsquo;re using IAM identity center: You&rsquo;ll need to use
<code>https://aws.amazon.com/SAML/Attributes/AccessControl:{ attr name}</code>
in Okta.</p>

<p>The bug? Do <strong>not</strong> use both <code>AccessControl</code> and <code>PrincipalTag</code>. Apparently the IdC freaks out during the SAML jank and wont do anything.</p>

<p>So in Okta, add this:</p>

<p><img src="/images/ec2-ssh/okta1.png"></p>

<p>Also while you&rsquo;re add it make sure you&rsquo;re using the nickName for the username.</p>

<p><img src="/images/ec2-ssh/okta2.png"></p>

<p>At this point go ahead and measure success by making sure that your user is in the <code>first.last</code> format and your <code>AssumeRoleWithSAML</code> call in Cloudtrail has the principalTags on the <code>requestParameters</code>. And by that I mean get angry and weep for 8 hours then find out you&rsquo;re hitting an invisible AWS bug. I&rsquo;ll wait.</p>

<p>Thanks for coming back.</p>

<p>Your user doesn&rsquo;t have to be in this format but it tracks with my needs and my SCP as well as my janky NSS code. Modify it if you need. You do you.</p>

<p><img src="/images/ec2-ssh/okta3.png"></p>

<pre><code>    "eventName": "AssumeRoleWithSAML",
    "awsRegion": "us-east-1",
    "sourceIPAddress": "5.3.9.1",
    "userAgent": "aws-internal/3 aws-sdk-java/1.12.451 blahblahblah",
    "requestParameters": {
        "sAMLAssertionID": "_94942c8c-6b73-4912-86f9-14c",
        "roleSessionName": "marcus.young",
        "principalTags": {
            "userName": "marcus.young"
        },
</code></pre>

<p>OK. Now that you&rsquo;re getting the metadata lets lock it down.</p>

<p>We&rsquo;re going to apply this SCP:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyPublicKeyNotOwnedBySelf",
      "Effect": "Deny",
      "Action": "ec2-instance-connect:SendSSHPublicKey",
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "ec2:osuser": "${aws:PrincipalTag/userName}"
        }
      }
    }
  ]
}
</code></pre>

<p>Very cool right?
What we&rsquo;ve done now is said &ldquo;Any call to <code>SendSSHPublicKey</code> has to have a <code>--instance-os-user</code> param that matches your <code>userName</code> principalTag that was set by okta</p>

<p>Prove it:</p>

<pre><code>$ aws ec2-instance-connect send-ssh-public-key \         
    --region us-east-1 \
    --availability-zone us-east-1a \
    --instance-id i-08843505c12cf9d7e \
    --instance-os-user myoung \
    --ssh-public-key file:///home/myoung/.ssh/asdf.pub | jq .

An error occurred (AccessDeniedException) when calling the SendSSHPublicKey operation: User: arn:aws:sts::1111111111:assumed-role/AWSReservedSSO_AdministratorAccess_e3aec0c44fb7c2e0/marcus.young is not authorized to perform: ec2-instance-connect:SendSSHPublicKey on resource: arn:aws:ec2:us-east-1:847713735871:instance/i-08843505c12cf9d7e with an explicit deny in a service control policy
</code></pre>



<pre><code>$ aws ec2-instance-connect send-ssh-public-key \
    --region us-east-1 \
    --availability-zone us-east-1a \
    --instance-id i-08843505c12cf9d7e \
    --instance-os-user marcus.young \
    --ssh-public-key file:///home/myoung/.ssh/asdf.pub | jq .

{
  "RequestId": "5b47d1c2-f8dc-447b-8a0f-6cd5bf060d89",
  "Success": true
}
</code></pre>

<p>Bam. Now you can&rsquo;t straight up ssh as others outright.</p>

<p>OK that part&rsquo;s done, but we still have another blocker: the user has to exist first.
That sucks if you have a thousand users. I don&rsquo;t know about you but I don&rsquo;t want to have to sit there and crawl Okta for users that may never ssh in.</p>

<p>OK So the way ec2-instance-connect software works (remember above about what SendSSHPublicKey does and ends it to the metadata endpoint):</p>

<ul>
<li>It adds some <code>AuthorizedKeysCommand</code> lines to <code>/etc/ssh/sshd_config</code></li>
<li>When A user tries to SSH in and fails first it will then hit <code>http://169.254.169.254/latest/meta-data/managed-ssh-keys/active-keys/{user name}</code></li>
<li>SSH Requires that user to exist. If it does, and the public key in memory from the metadata endpoint matches their private key: all good come on in bro</li>
</ul>


<p>So we can&rsquo;t rely on crawling, we can&rsquo;t rely on ssh, but we <strong>can</strong> rely on <a href="https://man7.org/linux/man-pages/man5/nss.5.html">nss</a> since thats what ssh relies on for <code>passwd</code>.
OK so I sound like an expert but I&rsquo;m not. A large chunk of this was hacked out by another person at work, but I&rsquo;m familiar enough. Don&rsquo;t lose hope in me yet.</p>

<p>So SSH talks to NSS. We can hook into NSS to do some logic and create a user like <a href="https://github.com/myoung34/ec2-instance-connect-libnss-create">this</a></p>

<p>So now in my user-data I have this sin:</p>

<pre><code>#cloud-config
packages:
 - gcc
 - unzip

write_files:
  - path: /usr/local/bin/install-nss-create.sh
    owner: root:root
    permissions: '0777'
    content: |
      #!/bin/bash
      pushd . &gt;/dev/null 2&gt;&amp;1
      cd /tmp
      curl -sL https://github.com/myoung34/ec2-instance-connect-libnss-create/archive/refs/heads/main.zip -o /tmp/nss-create.zip
      unzip nss-create.zip
      cd ec2-instance-connect-libnss-create-main
      make
      make install
      sed -i.bak 's/^passwd:.*sss files$/passwd:     sss files create files/g' /etc/nsswitch.conf
      popd &gt;/dev/null 2&gt;&amp;1

runcmd:
  - [/usr/local/bin/install-nss-create.sh]
</code></pre>

<p>I basically compile my nss garbage and add <code>create files</code> to <code>passwd</code> in <code>/etc/nsswitch.conf</code></p>

<p>This means that when a user attempts to SSH it will hook in my C code that calls a script
That script does a
<code>curl http://169.254.169.254/latest/meta-data/managed-ssh-keys/active-keys/{user name}</code>.</p>

<p>If it gets a 200-300 code (the user has put a public key in there - remember this only works for 60s) then it will create the user.</p>

<p>After <code>create</code> I still have <code>files</code> so that SSH will go back and look them back up in <code>/etc/passwd</code>. So assuming they ran the <code>aws ec2-instance-connect send-ssh-public-key</code> command then tried to ssh within 60 seconds: they&rsquo;ll have a user with a matching public key and they&rsquo;ll get in.</p>

<p>Bonus: we should be able to do this to get into private instances.</p>

<p>For this proof I&rsquo;m going to assume you have a public instance <code>1.2.3.4</code> and a private instance you want to reach <code>10.0.0.50</code></p>

<p>Let&rsquo;s make your <code>~/.ssh/config</code> file look like this:</p>

<pre><code>Host my-bastion
  IdentityFile ~/.ssh/asdf
  Hostname 1.2.3.4
  User marcus.young

Host private-vps
  IdentityFile ~/.ssh/asdf
  ProxyJump my-bastion
  StrictHostKeyChecking no
  Hostname 10.0.0.50
  User marcus.young
  UserKnownHostsFile /dev/null
</code></pre>

<p>Let&rsquo;s jump to that <code>private-vps</code> by using the <code>my-bastion</code> as a jump host.
Hint: You&rsquo;ll need to make 2 SendSSHPublicKey calls. One to the public (so you can get into it) and one to the private (so you can also get into it)</p>

<p>First let&rsquo;s prove we can&rsquo;t just get in. So you know I ain&rsquo;t lyin.</p>

<pre><code>$ ssh my-bastion 
marcus.young@1.2.3.4: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).

$ ssh private-vps
marcus.young@1.2.3.4: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
kex_exchange_identification: Connection closed by remote host
Connection closed by UNKNOWN port 65535
</code></pre>

<p>Told you. Let&rsquo;s push the key to both. (remember you have 60s. obviously id write a tool for this later).</p>

<pre><code>$ aws ec2-instance-connect send-ssh-public-key \
    --region us-east-1 \
    --availability-zone us-east-1a \
    --instance-id i-08e2c277f1ea9a99a \
    --instance-os-user marcus.young \
    --ssh-public-key file:///home/myoung/.ssh/asdf.pub | jq .Success
true

$ aws ec2-instance-connect send-ssh-public-key \
    --region us-east-1 \
    --availability-zone us-east-1a \
    --instance-id i-004b083d55a6d76c7 \
    --instance-os-user marcus.young \
    --ssh-public-key file:///home/myoung/.ssh/asdf.pub | jq .Success
true

$ ssh 10.0.0.50
Last login: Tue Apr 25 19:54:31 2023 from ip-10-0-0-50.ec2.internal

   __|  __|  __|
   _|  (   \__ \   Amazon Linux 2 (ECS Optimized)
 ____|\___|____/

For documentation, visit http://aws.amazon.com/documentation/ecs
1 package(s) needed for security, out of 3 available
Run "sudo yum update" to apply all updates.```
</code></pre>

<p>Very cool. So what did we accomplish?</p>

<ul>
<li>New instances have absolutely zero requirements on anything other than that very early PoC libnss module. Ideally I could package that up.</li>
<li>We have the normal SSH auditing story. auth.log lines up. We have cloudtrail logs that tell us people are ssh&#8217;ing in and where.</li>
<li>We have JIT users.</li>
<li>We use the same IAM stories for ssh. If I have 8 accounts I know that you need access to an AWS account to get to the instances into it. We could go further and add more principal tags to say that you can&rsquo;t do sudo unless you have a tag, or that you can&rsquo;t ssh at all if you don&rsquo;t have a certain department. This  is <em>leagues</em> better than some of the paid software I&rsquo;ve tested.</li>
<li>We increased no load to any custom tooling. Teleport has etcd, an SSH CA has some unique problems to solve, etc.</li>
<li>Our onboarding and offboarding story for ssh is now the same as IAM</li>
<li>In an incident etc we can simply block SendSSHPublicKey for suspicious users and <code>pkill</code> sshd for the named user (a win over shared users) on the bastion and effectively kill them off everywhere.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vault as a CA for ECS containers using Terraform (Part 2)]]></title>
    <link href="https://marcyoung.us/post/docker-vault-ca-part2"/>
    <updated>2017-12-24T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/docker-vault-ca-part2</id>
    <content type="html"><![CDATA[<p>Now for some cool snippets on how to automatically request certs from the Intermediary we created in Part 1<!-- more --></p>

<h2>Dockerfile</h2>

<p>An example of how I typically get Vault into my base Container:</p>

<pre><code>FROM debian:jessie
ENV VAULT_VERSION 0.8.1
ENV DUMBINIT_VERSION 1.2.0

RUN apt-get update &amp;&amp; apt-get install -y \
        apt-transport-https \
        ca-certificates \
        wget \
        unzip \
        jq \
        curl \
    nginx \
    --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN curl -L --silent -o vault.zip https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip &amp;&amp; \
  unzip vault.zip &amp;&amp; \
  mv vault /usr/local/bin/vault &amp;&amp; \
  chmod +x /usr/local/bin/vault

RUN curl -L --silent -o /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v${DUMBINIT_VERSION}/dumb-init_${DUMBINIT_VERSION}_amd64 &amp;&amp; \
  chmod +x /usr/local/bin/dumb-init

RUN mkdir -p /opt/ssl
COPY docker-entrypoint.sh /
RUN chmod +x /docker-entrypoint.sh
ENTRYPOINT ["/usr/local/bin/dumb-init", "--"]
CMD ["/docker-entrypoint.sh"]
</code></pre>

<h2>The Entrypoint</h2>

<pre><code>#!/bin/bash

export SSL_PATH=/opt/ssl

EC2_AVAIL_ZONE=$(curl -s --max-time 5 https://169.254.169.254/latest/meta-data/placement/availability-zone)
if [[ $? -eq 0 ]]; then
  # shellcheck disable=SC2001,SC2006
  EC2_REGION="`echo \"$EC2_AVAIL_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"
  export AWS_DEFAULT_REGION=$EC2_REGION
  export AWS_REGION=$EC2_REGION
fi

[[ -z "${VAULT_TOKEN}" ]] &amp;&amp; vault auth -method=aws &gt;/dev/null

VAULT_JSON="$(vault write -format=json cuddletech_ops/issue/jenkins common_name=jenkins ttl=15551999)"

echo "$VAULT_JSON" | jq -r .data.private_key &gt; ${SSL_PATH}/key.pem
echo "$VAULT_JSON" | jq -r .data.certificate &gt; ${SSL_PATH}/cert.pem
echo "$VAULT_JSON" | jq -r .data.issuing_ca  &gt; ${SSL_PATH}/ca.pem
cat ${SSL_PATH}/cert.pem ${SSL_PATH}/ca.pem  &gt; ${SSL_PATH}/bundle.pem

# run your command and point to the certs generated above. Example in nginx:
#        ssl                  on;
#        ssl_certificate      /opt/ssl/bundle.pem;
#        ssl_certificate_key  /opt/ssl/key.pem;
</code></pre>

<h2>Terraforming the certificate issuer</h2>

<p>If you somehow get the previous part to run, it will fail because you have to create the role in Vault before you can just start generating certs off it. In the <a href="https://cuddletech.com/?p=959">cuddletech guide</a> this is under <code>Requesting a Certificate for a Web Server</code> as:</p>

<pre><code>$ vault write cuddletech_ops/roles/web_server \
&gt; key_bits=2048 \
&gt; max_ttl=8760h \
&gt; allow_any_name=true
Success! Data written to: cuddletech_ops/roles/web_server
</code></pre>

<p>But what if I told you, you could do that in terraform?</p>

<pre><code>variable "role" {
}

resource "vault_generic_secret" "pki_role" {
  path = "cuddletech_ops/roles/${var.role}"

  data_json = &lt;&lt;EOT
{
  "key_bits": "2048",
  "max_ttl": "8760h",
  "allow_any_name": "true"
}
EOT
}
</code></pre>

<p>If you add that with your ECS task along with the vault role and policy from the previous post about Vault Anything in Terraform, all your dependencies are met and kept in code!</p>

<p>You can now generate certificates off your PKI backend and grab secrets all in one go!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vault as a CA for ECS containers using Terraform (Part 1)]]></title>
    <link href="https://marcyoung.us/post/docker-vault-ca-part1"/>
    <updated>2017-12-24T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/docker-vault-ca-part1</id>
    <content type="html"><![CDATA[<p>In this I&rsquo;ll show some examples on how to leverage HashiCorp Vault as an awesome CA. Note this part is boring and 80% copy/paste from a cuddletech blog. <!-- more --></p>

<h2>Prerequisites</h2>

<ol>
<li>Vault running as a server</li>
<li>Vault CLI to match the server</li>
</ol>


<h2>Create the Root cert/key</h2>

<p>(This is actually mostly a clone from <a href="https://cuddletech.com/?p=959">here</a> but I want to make sure it never disappears)</p>

<p><em>Note</em>: You should not let the vault server generate the root CA, you should generate it and import it. I just havent figured out how to do that yet. I will update this post when I get around to it. <a href="https://en.wikipedia.org/wiki/Offline_root_certificate_authority">More info on that here</a></p>

<ol>
<li>Mount the PKI backend for the root</li>
</ol>


<pre><code>$ vault mount -path=cuddletech -description="Cuddletech Root CA" -max-lease-ttl=87600h pki
Successfully mounted 'pki' at 'cuddletech'!
$ vault mounts
Path         Type       Default TTL  Max TTL    Description
cubbyhole/   cubbyhole  n/a          n/a        per-token private secret storage
cuddletech/  pki        system       315360000  Cuddletech Root CA
secret/      generic    system       system     generic secret storage
sys/         system     n/a          n/a        system endpoints used for control, policy and debugging 
</code></pre>



<pre><code>$ vault write cuddletech/root/generate/internal \
&gt; common_name="Cuddletech Root CA" \
&gt; ttl=87600h \
&gt; key_bits=4096 \
&gt; exclude_cn_from_sans=true
Key             Value
---             -----
certificate     -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
..
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
expiration      1783411981
issuing_ca      -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
...
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
serial_number   0d:78:88:dc:60:f3:3f:62:1b:43:d2:1a:b4:54:82:bf:d3:ea:fe:58
</code></pre>

<p>Verify</p>

<pre><code>$ curl -s https://localhost:8200/v1/cuddletech/ca/pem | openssl x509 -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            0d:78:88:dc:60:f3:3f:62:1b:43:d2:1a:b4:54:82:bf:d3:ea:fe:58
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Cuddletech Root CA
        Validity
            Not Before: Jul  9 08:12:31 2016 GMT
            Not After : Jul  7 08:13:01 2026 GMT
        Subject: CN=Cuddletech Root CA
...
</code></pre>

<p>Fix the URL for accessing the CA/CRL URLs</p>

<pre><code>$ vault write cuddletech/config/urls issuing_certificates="https://10.0.0.22:8200/v1/cuddletech
Success! Data written to: cuddletech/config/urls
</code></pre>

<h2>Create the Intermediate cert/key</h2>

<p>Mount the PKI for the Intermediate (similar to root)</p>

<pre><code>vault mount -path=cuddletech_ops -description="Cuddletech Ops Intermediate CA" -max-lease-ttl=26280h pki
Successfully mounted 'pki' at 'cuddletech_ops'!

$ vault mounts
Path             Type       Default TTL  Max TTL    Description
cubbyhole/       cubbyhole  n/a          n/a        per-token private secret storage
cuddletech/      pki        system       315360000  Cuddletech Root CA
cuddletech_ops/  pki        system       94608000   Cuddletech Ops Intermediate CA
</code></pre>

<p>Generate the intermediate CSR</p>

<pre><code>$ vault write cuddletech_ops/intermediate/generate/internal \
&gt;  common_name="Cuddletech Operations Intermediate CA"
&gt;  ttl=26280h \
&gt;  key_bits=4096 \
&gt;  exclude_cn_from_sans=true
Key     Value
---     -----
csr     -----BEGIN CERTIFICATE REQUEST-----
MIICuDCCAaACAQAwMDEuMCwGA1UEAxMlQ3VkZGxldGVjaCBPcGVyYXRpb25zIElu
dGVybWVkaWF0ZSBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALt8
...
hD8cpHTXqjKExYWKc/rQDgjw9+RNDdb45xszDagrgFgNPqI9i0fNh9jViMmjUiTc
PQTZS4XxIoRrx1/xVHJ4Qm++ntLPVCvzjMZafg==
-----END CERTIFICATE REQUEST-----
</code></pre>

<p>Cut and paste that CSR into a new file cuddletech_ops.csr. The reason we output the file here is so we can get it out of one backend and into another and then back out.</p>

<pre><code>$ vault write cuddletech/root/sign-intermediate \
&gt;  csr=@cuddletech_ops.csr \
&gt;  common_name="Cuddletech Ops Intermediate CA" \
&gt;  ttl=8760h
Key             Value
---             -----
certificate     -----BEGIN CERTIFICATE-----
MIIEZDCCAkygAwIBAgIUHuIhRF3tYtfoZiAFdjcCtQpMR+cwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4Mjkz
...
UtI2b/AamAqf340eRKmSdEh4WypB4JR+t259YA45w2j4mS+rxREycEk4YosR/vUs
jekMiq57yNq7h8eOTrnOulJxazbVrYGb
-----END CERTIFICATE-----
expiration      1470645002
issuing_ca      -----BEGIN CERTIFICATE-----
MIIFKzCCAxOgAwIBAgIUDXiI3GDzP2IbQ9IatFSCv9Pq/lgwDQYJKoZIhvcNAQEL
BQAwHTEbMBkGA1UEAxMSQ3VkZGxldGVjaCBSb290IENBMB4XDTE2MDcwOTA4MTIz
..
1FRGlwHUg+6IIZBVIapzivLc6pAvLFPxQlQvT5CNHPk91zwyNQ9ZX2PzatdajUnd
axscmLdVE2HTB87W1H77iKKN8n9Xne//LUidxVX0Kg==
-----END CERTIFICATE-----
serial_number   1e:e2:21:44:5d:ed:62:d7:e8:66:20:05:76:37:02:b5:0a:4c:47:e7
</code></pre>

<p>Now that we have a Root CA signed cert, we’ll need to cut-n-paste this certificate into a file we’ll name cuddletech_ops.crt and then import it into our Intermediate CA backend:</p>

<pre><code>$ vault write cuddletech_ops/intermediate/set-signed \
&gt; certificate=@cuddletech_ops.crt
Success! Data written to: cuddletech_ops/intermediate/set-signed
</code></pre>

<p>Awesome! Lets verify:</p>

<pre><code>$ curl -s https://localhost:8200/v1/cuddletech_ops/ca/pem | openssl x509 -text | head -20
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            76:12:53:41:be:18:98:2c:a1:51:4a:f8:f0:bd:b4:a3:44:7e:74:59
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Cuddletech Root CA
        Validity
            Not Before: Jul  9 09:23:39 2016 GMT
            Not After : Jul  9 09:24:09 2017 GMT
        Subject: CN=Cuddletech Ops Intermediate CA
 ...
</code></pre>

<p>The last thing we need to do is set the CA &amp; CRL URL’s for accessing the CA:</p>

<pre><code>$ vault write cuddletech_ops/config/urls \
&gt; issuing_certificates="https://10.0.0.22:8200/v1/cuddletech_ops/ca" \
&gt; crl_distribution_points="https://10.0.0.22:8200/v1/cuddletech_ops/crl"
Success! Data written to: cuddletech_ops/config/urls
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vault Roles/Policies in Terraform]]></title>
    <link href="https://marcyoung.us/post/vault-anything-in-terraform"/>
    <updated>2017-12-23T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/vault-anything-in-terraform</id>
    <content type="html"><![CDATA[<p>Now we get to the fun part and put everything we need from Vault in terraform. <!-- more --></p>

<p>If you&rsquo;ve used ECS and Vault with the <a href="https://www.vaultproject.io/docs/auth/aws.html">aws auth backend</a> you may have found a bit of a snag in terms of automation.</p>

<p>Lets say you want to create a new ECS container to run. You want that task to get specific secrets in vault by role. You write the code, terraform, etc and when it runs you get:</p>

<pre><code>URL: GET https://server:8200/v1/auth/aws-ec2/role/somerole
Code: 400. Errors:

* entry for role somerole not found
</code></pre>

<p>So you deployed something but now you have to create a policy, a role, etc in vault via the CLI, all the while your task is throttling.</p>

<p>Little known fact: the terribly-named <code>vault_generic_secret</code> from the <a href="https://www.terraform.io/docs/providers/vault/index.html">provider docs</a> can do create roles/policies in addition to secrets (hence the bad name).</p>

<p>In this part 2 I&rsquo;ll show you how to automate the next step of your ECS task into terraform so there&rsquo;s no race condition.</p>

<h2>Prerequisites</h2>

<ol>
<li>Vault running as a server</li>
<li>Vault CLI to match the server</li>
<li>Terraform</li>
</ol>


<h2>Create a vault policy in terraform</h2>

<pre><code>resource "vault_generic_secret" "policy" {
  path = "sys/policy/${aws_iam_role.neutrona_wand.name}"

  data_json = &lt;&lt;EOT
{
  "name": "${aws_iam_role.neutrona_wand.name}",
  "rules": "path \"secret/ops/*\" {\n  policy = \"read\"\n}\n\npath \"secret/common/*\" {\n  policy = \"read\"\n}"
}

EOT
}
</code></pre>

<h2>Create an AWS auth role in terraform</h2>

<pre><code>provider "vault" {}

resource "aws_iam_role" "neutrona_wand" {
  name = "neutrona_wand"
  path = "/lambda/"

  assume_role_policy = &lt;&lt;POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": [
          "lambda.amazonaws.com"
        ]
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}

# The ugly replace in 'bound_iam_principal_arn' is to account for removing the path from the role. 
# If you look, my IAM role is /lambda/, so the arn would be 'arn:aws:iam::1234567890:role/lambda/neutrona_wand'
# While the 'bound_iam_principal_arn' wants the non-path version: 'arn:aws:iam::1234567890:role/neutrona_wand'
resource "vault_generic_secret" "role" {
  path = "auth/aws/role/${aws_iam_role.neutrona_wand.name}"

  data_json = &lt;&lt;EOT
{
  "bound_iam_principal_arn": "${replace(aws_iam_role.neutrona_wand.arn, "/:role\\/[a-zA-Z\\/]*\\//", ":role\\/")}",
  "policies": "default,${aws_iam_role.neutrona_wand.name},slack",
  "max_ttl":"300",
  "resolve_aws_unique_ids": "false"
}
EOT
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Up and Running With Bless and Enforced MFA - Part 2]]></title>
    <link href="https://marcyoung.us/post/bless-part2"/>
    <updated>2017-06-20T00:00:00+00:00</updated>
    <id>https://marcyoung.us/post/bless-part2</id>
    <content type="html"><![CDATA[<p>Part 2 of deploying Bless will focus on enforcing MFA and using Lyft’s client. <!-- more --></p>

<p>The problem at this point is that anyone in the ops group can bounce without MFA enforced. We can set up the lyft client, which enforces MFA, but if we dont make other changes, there’s no enforcement of MFA. ie you can still call the netflix client and bounce right in.</p>

<p>The other issue is that Netflix’s bless holds true the idea of a bastion host. In-house, we have IPSec tunnels from our VPC’s to our internal network, so I dont need to bounce through a bastion. So I want anyone to be able to bounce, given that they have permissions (ops) and they used MFA. Lyft’s doesn’t enforce the bastion concept, which is nice.</p>

<h1>Prerequisites</h1>

<p>In the IAM console in AWS, enable an MFA device for your user.</p>

<h2>Setting up the client</h2>

<p>I wrote a wrapper to simplify this. Feel free to not like it, but I put this in my <code>~/.bash_aliases</code> to make it easier:</p>

<pre><code>function bounce() {
  if [[ ! -d ~/.blessclient ]]; then
    pushd . &gt;/dev/null
    git clone https://github.com/lyft/python-blessclient.git ~/.blessclient
    cd $_
    make
    popd &gt;/dev/null
  fi
  INTERNAL_IP=$(ifconfig | grep inet | grep 192 | awk '{ print $2}')
  # This gets weird if you have wired and wifi hooked up (sometimes i do). It might allow the wrong IP and give you a crap error messsage.
  if [[ $(ifconfig | grep inet | grep 192 | awk '{ print $2}' | wc -l) -gt 1 ]] &amp;&amp; [[ -z ${BLESSFIXEDIP} ]]; then
    echo "More than one internal ip found. Disable a network interface or provide it manually via env var 'BLESSFIXEDIP'"
  else
    FILE="$(mktemp)"
    rm -rf ${FILE}*
    ssh-keygen -f ${FILE} -N ""
    BLESS_IDENTITYFILE=${FILE} BLESSFIXEDIP=${BLESSFIXEDIP:-${INTERNAL_IP}} ~/.blessclient/blessclient.run --host $1 --nocache --config ~/.blessclient/blessclient.cfg --region EAST &amp;&amp; ssh -i ${FILE} ec2-user@$1
  fi
}
</code></pre>

<p>Now you can bounce with:</p>

<pre><code>$ bounce 10.0.3.7
Generating public/private rsa key pair.
Your identification has been saved in /var/folders/5j/n9trdvbn3qqdtkshkpvnrkhh0000gp/T/tmp.s2ZdeuJD.
Your public key has been saved in /var/folders/5j/n9trdvbn3qqdtkshkpvnrkhh0000gp/T/tmp.s2ZdeuJD.pub.
The key fingerprint is:
SHA256:CizQBI/ROY8QIeiRjt7rY65jPc/oJDL3MA marcyoung@Admins-MacBook-Pro.local
The key's randomart image is:
+---[RSA 2048]----+
|B*+.             |
|++o              |
|=..              |
|.o.*.            |
|. o  S           |
|...+ . .         |
| E .o.           |
|oo= + .          |
|.=*              |
+----[SHA256]-----+
Enter your AWS MFA code: 477821
Requesting certificate for your public key (set BLESSQUIET=1 to suppress these messages)
Finished getting certificate.
Last login: Fri Jun 16 20:14:52 2017 from ip-192-168-1-124.ec2.internal

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2017.03-release-notes/
No packages needed for security; 2 packages available
Run "sudo yum update" to apply all updates.
[marc@server ~]$
</code></pre>

<p>Nice! But again, MFA is used, not enforced. Lets change that.</p>

<p>I hinted at my group policy in part 1 for <code>ops</code> that looked like:</p>

<pre><code>resource "aws_iam_group_policy" "bless-client" {
    name = "bless-client_iam_policy"
    group = "${aws_iam_group.ops.id}"
    policy = &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Action": "sts:AssumeRole",
        "Resource": [
          "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
        ]
      },
      {
        "Action": "kms:Encrypt",
        "Effect": "Allow",
        "Resource": [
          "${data.terraform_remote_state.kms.ops_bless_arn}"
        ],
        "Condition": {
          "StringEquals": {
            "kms:EncryptionContext:user_type": "user",
            "kms:EncryptionContext:from": "$${aws:username}"
          }
        }
      }
    ]
}
EOF
}
</code></pre>

<p>But in actuality, it should look more like:</p>

<pre><code>resource "aws_iam_group_policy" "bless-client" {
    name = "assume-bless-role"
    group = "${aws_iam_group.ops.id}"
    policy = &lt;&lt;EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": [
        "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
      ],
      "Condition": {
        "Bool": {
          "aws:MultiFactorAuthPresent": "true"
        }
      }
    },
    {
      "Effect": "Deny",
      "Action": "sts:AssumeRole",
      "Resource": [
        "${data.terraform_remote_state.bless_client_iam.bless-client-role-arn}"
      ],
      "Condition": {"BoolIfExists": {"aws:MultiFactorAuthPresent": false}}
    },
    {
      "Action": "kms:Encrypt",
      "Effect": "Allow",
      "Resource": [
        "${data.terraform_remote_state.kms.ops_bless_arn}"
      ],
      "Condition": {
        "StringEquals": {
          "kms:EncryptionContext:user_type": "user",
          "kms:EncryptionContext:from": "$${aws:username}"
        },
        "Bool": {
          "aws:MultiFactorAuthPresent": "true"
        }
      }
    },
    {
      "Action": "kms:Encrypt",
      "Effect": "Deny",
      "Resource": [
        "${data.terraform_remote_state.kms.ops_bless_arn}"
      ],
      "Condition": {
        "StringEquals": {
          "kms:EncryptionContext:user_type": "user",
          "kms:EncryptionContext:from": "$${aws:username}"
        },
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": false
        }
      }
    }
  ]
}
EOF
}
</code></pre>

<p>What did we do? We basically said ops users can assume the bless-client role or call kms:Encrypt on that key, but only if they used MFA. If you try the original bless client now, it will fail.</p>
]]></content>
  </entry>
  
</feed>
